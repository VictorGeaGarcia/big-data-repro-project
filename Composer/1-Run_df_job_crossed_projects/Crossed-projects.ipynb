{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will need to load our DAG file <font color=\"red\">(\"DAG code name\")</font> into the DAG folder <font color=red>(gs://your-env-bucket/dags/)</font>. Before we do this let's set up some variables to make our code cleaner:\n",
    "- <font color=red> Variables:--projectA /projectB / ?? </font>\n",
    "     We will need to load our DAG file <font color=\"red\">(\"DAG code name\")</font> into the DAG folder <font color=red>(gs://your-env-bucket/dags/)</font>. Before we do this let's set up some variables to make our code cleaner:\n",
    "- <font color=red> Variables:--projectA /projectB / ?? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README.md\n",
    "<h2>Running Dataflow job in project A from Composer in project B</h2>\n",
    "\n",
    "Being able to use a Composer environment running in project B to launch a Dataflow Job in different projects (Project A i.e).\n",
    "\n",
    "Basically we want to have a central project from where we use Composer to launch Dataflow jobs to different projects.\n",
    "\n",
    "\n",
    "In this particular use case we have the following requirements:\n",
    "<ul>\n",
    "<li> Project B must have Composer API enabled and Dataflow API disabled.</li>\n",
    "    <li> Project A must have Composer API disabled and Dataflow API enabled.</li>\n",
    "<li> We don't want to  make use of Project B SA. So we will create a SA in Project A that will be the one used to launch the DF jobs.</li>\n",
    "<li><a href=https://airflow.apache.org/integration.html#dataflowpythonoperator>We will use DataflowPythonOperator and its gcp_conn_id parameter </a> </li>\n",
    "</ul>\n",
    "\n",
    "STEPS:\n",
    "\n",
    "Before you begin:\n",
    "\n",
    "You'll use the two files that are in in this repo \"DAG_using_DFPythonOperator.py\" and \"DAG_using_BashOperator.py\". You'll need to do the following changes to those files:\n",
    "\n",
    "Change <>\n",
    "\n",
    "1. Create Project B and enable Composer API.\n",
    "2. Create Composer Environment in project B.\n",
    "3. Create Project A and enable Dataflow API.\n",
    "- Create Bucket inside Project A (to place staging,temp and output files from Dataflow Job).\n",
    "\n",
    "4. Create SA in project A <SA-name@project-id.iam.gserviceaccount.com> and grant it the appropriate permissions so that it can launch a DF job:\n",
    "<ul>\n",
    "<li>Compute Admin</li>\n",
    "<li>Dataflow Admin</li>\n",
    "<li>Dataflow worker</li>\n",
    "<li>Storage Object Admin</li>\n",
    "</ul>\n",
    "\n",
    "5. Create a keyjson file for your SA <SA-name@project-id.iam.gserviceaccount.com>, download it and keep it. \n",
    "6. To use the \"gcp_conn_id\" parameter in DataflowPythonOperator we need to do the following (https://cloud.google.com/composer/docs/how-to/managing/connections#creating_new_airflow_connections):\n",
    "\n",
    "- Go to your Airflow UI --> Admin --> Connections\n",
    "- Click on Create\n",
    "- Enter the following values:\n",
    "\n",
    "-- Conn Id: \"<name-you'll-use-in-gcp_conn_id-parameter>\"\n",
    "\n",
    "-- Conn Type: Choose Google Cloud Platform\n",
    "\n",
    "-- Project ID: \"your-project-A-ID\"\n",
    "    \n",
    "-- Keyfile Path or Keyfile JSON: (Either upload the previously generated SA keyfile in step 5 to Composer Cloud Storage bucket path (/home/airflow/gcs/data/<your-dataflow-SA-key.json>) and add the path to Keyfile Path field or add the contents of the file directly to Keyfile JSON).\n",
    "\n",
    "7. You'll need to load the code file \"DAG_using_DFPythonOperator.py\" to your DAG folder, but before doing so perform the following changes to the file:\n",
    "- Change \"staging-bucket\" with the name of the bucket you created in step 3.\n",
    "- Change \"project-A\" to your ProjectA id created in Step 3.\n",
    "- Load the \"wordcount.py\" file in this repo to your data/ folder in your Cloud Storage bucket associated to your Cloud Composer environment (run \"gcloud composer environments describe \"your-cloud-composer-env\" --location \"your-composer-env-location\"\" to see your bucket name) \n",
    "- Change the gcp_conn_id name to the one you have given to the connection created in step 6.\n",
    "\n",
    "8. This will not work and it should throw some error like the following:\n",
    "\n",
    "\"IOError: Could not upload to GCS path gs://\\\\<staging-bucket\\\\>/.../... access denied. Please verify that credentials are valid and that you have write access to the specified path.\"\n",
    "\n",
    "My staging bucket is in ProjectA, and the SA which should be used for the connection has been granted StorageAdmin permissions in that project. Still it doesn't work due to <a href=https://issues.apache.org/jira/browse/AIRFLOW-2009>this issue</a>\n",
    "\n",
    "9. Although not that neat, we can use the following workaround: \n",
    "\n",
    "Only using BashOperator to launch the Job without using the DataflowPythonOperator. To use this approach load the file \"DAG_using_BashOperator.py\" to your DAG folder. Before doing so do:\n",
    "\n",
    "\n",
    "- Locate the keyjson file created in step 5 to your data/ folder in your Cloud Storage bucket associated to your Cloud Composer environment (we will need to export our GOOGLE_APPLICATION_CREDENTIALS to this path). Now change \"your-project-A-sa.json\" to your KeyJson filename.\n",
    "- Change \"staging-bucket\" with the name of the bucket you created in step 3.\n",
    "- Change \"project-A\" to your ProjectA-id created in Step 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAG file:\n",
    "\n",
    "import datetime\n",
    " \n",
    "from airflow import models\n",
    "from airflow.contrib.operators.dataflow_operator import DataFlowPythonOperator\n",
    "from airflow.operators import BashOperator\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    " \n",
    " \n",
    "default_args = {\n",
    "    'start_date':yesterday\n",
    "}\n",
    " \n",
    "with models.DAG(\n",
    "    'dataflow_python_gcp_conn_id',\n",
    "    schedule_interval=None,\n",
    "    default_args=default_args) as dag:\n",
    "    \n",
    "        bash_nothing = BashOperator(task_id='nothing_2',bash_command='echo nothing')\n",
    "        \n",
    "        run_dataflow_python = DataFlowPythonOperator(\n",
    "\t\t\ttask_id='df-conn-gcp-id-from-json',\n",
    "\t\t\tpy_file='/home/airflow/gcs/data/wordcount.py',\n",
    "\t\t\toptions={'runner':'DataflowRunner',\n",
    "                                 'output':'gs://staging-bucket-hijo-project/out',\n",
    "                                 'temp_location':'gs://staging-bucket-hijo-project/teemp',\n",
    "                                 'staging_location':'gs://staging-bucket-hijo-project/staging',\n",
    "                                 'project':'hijo-project'},\n",
    "                        gcp_conn_id='cloud-dataflow-hijo-project-from-location')\n",
    "        bash_nothing >> run_dataflow_python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
